# âœˆï¸ Multimodal Airline Assistant  

An **AI-powered Airline Assistant** that helps users book flights, check baggage allowances, and fetch booking details using **Ollama** for LLM reasoning and **Gradio** for an interactive multimodal UI.  

It supports:  
- ğŸ—£ï¸ **Natural Language Queries** (e.g., *â€œFind flights from Bangalore to Delhi tomorrowâ€*)  
- ğŸ–¼ï¸ **Vision Q&A** (upload boarding passes, flight tickets, or images for AI-based answers)  
- ğŸ§¾ **PNR Lookup** (fetch booking details from PNR codes)  
- ğŸ’ **Baggage Allowance Queries** (by cabin class, status, domestic/international)  

---

## ğŸš€ Features  
- ğŸ’¬ **Conversational AI** with Ollama (chat-based airline assistant)  
- ğŸ›« **Flight Search** (origin, destination, date, cabin, passengers)  
- ğŸŸ **PNR Fetch** (lookup booking details)  
- ğŸ“¦ **Baggage Allowance** (economy, business, frequent flyer tiers)  
- ğŸ–¼ï¸ **Vision AI** (analyze flight-related images like boarding passes)  
- ğŸŒ **Gradio UI** for a seamless chat interface  

---

## ğŸ› ï¸ Tech Stack  
- **Python** (Backend)  
- **Gradio** (Frontend UI)  
- **Ollama** (Local LLM API)  
- **Requests / JSON** for API calls  
- **Torch + Faster Whisper** (optional for speech)  

---

## Screenshots


## ğŸ“‚ Project Structure  
```
Airline-Agent/
â”‚â”€â”€ app.py # Main Gradio app
â”‚â”€â”€ ollama_client.py # Wrapper for Ollama API (chat + vision)
â”‚â”€â”€ requirements.txt # Dependencies
â”‚â”€â”€ README.md # Project documentation
```

---

## âš¡ Installation  

1ï¸âƒ£ Clone the repo  
```bash
git clone https://github.com/your-username/airline-assistant.git
cd airline-assistant
```
2ï¸âƒ£ Create & activate virtual environment
```
python -m venv venv
source venv/bin/activate   # (Linux/Mac)
venv\Scripts\activate      # (Windows)
```
3ï¸âƒ£ Install dependencies
```
pip install -r requirements.txt
```
4ï¸âƒ£ Install & run Ollama locally
```
ğŸ‘‰ Download Ollama
ğŸ‘‰ Run your preferred model (e.g., llama3)
ollama run llama3
```
â–¶ï¸ Usage

Run the Gradio app:
```
python app.py
```
Open in browser: http://127.0.0.1:7860

---

âš ï¸ Troubleshooting

404 Ollama error â†’ Make sure Ollama is running (ollama serve)

OMP Error #15 â†’ Add at top of app.py:
```
import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
os.environ["OMP_NUM_THREADS"] = "1"
```

Model not found â†’ Pull model first:
```
ollama pull llama3
```

ğŸ‘¨â€ğŸ’» Author

Dhanush Moolemane 

---
